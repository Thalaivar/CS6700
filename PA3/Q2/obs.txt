epsilon constant = 0.1
- no trend, lots of oscillations, but max steps are reached for more and more episodes
- increasing lr to 1e-3, too many oscillations no improvement at all after 4000 episodes
- decreasing lr to 1e-5, very slow but monotonic increase in avg reward. maybe decreasing lr has dampening effect, sudden drop, overall lr is too low
- lr set to 1e-4, achieved success in 6k episodes, but not a monotonic increase

trying to get monotonic increase now
- (lr = 1e-4) increased batch size from 64 to 128, tremendous improvement, success within 1k episodes, maybe increasing lr is an option? letting it run for too long is bad, sudden drop in performance, 
- increased lr to 0.01 and batch sz to 512, but not stable. increasing repla buffer size to 100k gives decent results
- decreasing lr to 0.001 with batch of 512 and replay buffer 10k, best results so far

trying to improve lr = 1e-3, batch_sz = 512, buffer = 10k
- eps decay = 0.999 eps_max = 0.5 eps_min = 0.05: too oscillatorys; eps decay = 0.9995 monotonic increase but sudden drop
- chaotic behaviour of candidate setting 1, for remedy: changing target net freq has no improvement



all the foll. are constant epsilon:
	batch size causes large oscillations when too small, catastrophic forgetting

	increasing hidden layer size, (64, 64), too many oscillations, reaches 200 fast but falls down (to ~ 180) too many times and no convergence, batchsz was 512 w buffer 10k, lr 1e-3
	same confg but batchsz 64, same amount of instability
	avg batch size, no change, so batch size has no effect on stability?
	batch size was not the problem, learning rate too high, 1e-4 gives good results (saving this conf , candidate 1x)
	increasing batch size reduces the number of oscillatons as compared to previous case (conf 2x)
	increasing buffer size reduces number of oscillations (conf 3x)
	
	soft-updates MASSIVELY improve performance. (conf 4x)
	